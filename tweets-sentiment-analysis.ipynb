{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n\nEDA, preprocessing and test evaluation included.\n\nModels used:\n1. Logistic Regression\n2. TextBlob\n3. Neural Network (Pytorch Lightning)","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pickle","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-20T11:16:04.411596Z","iopub.execute_input":"2022-07-20T11:16:04.411988Z","iopub.status.idle":"2022-07-20T11:16:04.416508Z","shell.execute_reply.started":"2022-07-20T11:16:04.411955Z","shell.execute_reply":"2022-07-20T11:16:04.415505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-07-20T11:16:06.105Z","iopub.execute_input":"2022-07-20T11:16:06.106047Z","iopub.status.idle":"2022-07-20T11:16:06.110678Z","shell.execute_reply.started":"2022-07-20T11:16:06.105995Z","shell.execute_reply":"2022-07-20T11:16:06.109782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load the data\ndata = pd.read_csv('../input/twitter-entity-sentiment-analysis/twitter_training.csv', header=None)\nval_data = pd.read_csv('../input/twitter-entity-sentiment-analysis/twitter_validation.csv', header=None)","metadata":{"execution":{"iopub.status.busy":"2022-07-20T11:16:07.922754Z","iopub.execute_input":"2022-07-20T11:16:07.923907Z","iopub.status.idle":"2022-07-20T11:16:08.221417Z","shell.execute_reply.started":"2022-07-20T11:16:07.923836Z","shell.execute_reply":"2022-07-20T11:16:08.220517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2022-07-20T07:43:45.969714Z","iopub.execute_input":"2022-07-20T07:43:45.970165Z","iopub.status.idle":"2022-07-20T07:43:46.005431Z","shell.execute_reply.started":"2022-07-20T07:43:45.970124Z","shell.execute_reply":"2022-07-20T07:43:46.00454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_data","metadata":{"execution":{"iopub.status.busy":"2022-07-20T07:43:55.029067Z","iopub.execute_input":"2022-07-20T07:43:55.029954Z","iopub.status.idle":"2022-07-20T07:43:55.044759Z","shell.execute_reply.started":"2022-07-20T07:43:55.029911Z","shell.execute_reply":"2022-07-20T07:43:55.043865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# columns names\ndata.columns = ['#', 'refers to', 'sentiment', 'text']\nval_data.columns = data.columns","metadata":{"execution":{"iopub.status.busy":"2022-07-20T11:16:37.970096Z","iopub.execute_input":"2022-07-20T11:16:37.970919Z","iopub.status.idle":"2022-07-20T11:16:37.975907Z","shell.execute_reply.started":"2022-07-20T11:16:37.970875Z","shell.execute_reply":"2022-07-20T11:16:37.974756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# global names\nTARGET = 'sentiment'","metadata":{"execution":{"iopub.status.busy":"2022-07-20T11:16:39.431381Z","iopub.execute_input":"2022-07-20T11:16:39.432351Z","iopub.status.idle":"2022-07-20T11:16:39.436882Z","shell.execute_reply.started":"2022-07-20T11:16:39.432313Z","shell.execute_reply":"2022-07-20T11:16:39.435764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2022-07-20T11:11:05.229145Z","iopub.execute_input":"2022-07-20T11:11:05.230163Z","iopub.status.idle":"2022-07-20T11:11:05.297853Z","shell.execute_reply.started":"2022-07-20T11:11:05.230118Z","shell.execute_reply":"2022-07-20T11:11:05.296966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T11:16:20.598185Z","iopub.execute_input":"2022-07-20T11:16:20.598769Z","iopub.status.idle":"2022-07-20T11:16:20.660809Z","shell.execute_reply.started":"2022-07-20T11:16:20.598729Z","shell.execute_reply":"2022-07-20T11:16:20.659705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Nans","metadata":{}},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-07-19T13:53:19.994764Z","iopub.execute_input":"2022-07-19T13:53:19.995145Z","iopub.status.idle":"2022-07-19T13:53:20.034452Z","shell.execute_reply.started":"2022-07-19T13:53:19.995113Z","shell.execute_reply":"2022-07-19T13:53:20.033688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T07:44:29.772539Z","iopub.execute_input":"2022-07-20T07:44:29.773028Z","iopub.status.idle":"2022-07-20T07:44:29.790592Z","shell.execute_reply.started":"2022-07-20T07:44:29.772986Z","shell.execute_reply":"2022-07-20T07:44:29.78989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 686 samples with no text. As text information is crutial for us, we are going to remove these samples for both EDA anf models fitting.","metadata":{}},{"cell_type":"code","source":"# drop samples with nans\ndata.dropna(inplace=True, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-07-20T11:16:27.312183Z","iopub.execute_input":"2022-07-20T11:16:27.312978Z","iopub.status.idle":"2022-07-20T11:16:27.351368Z","shell.execute_reply.started":"2022-07-20T11:16:27.312937Z","shell.execute_reply":"2022-07-20T11:16:27.350483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Text stats","metadata":{}},{"cell_type":"code","source":"texts = data['text']","metadata":{"execution":{"iopub.status.busy":"2022-07-20T11:16:43.786171Z","iopub.execute_input":"2022-07-20T11:16:43.787309Z","iopub.status.idle":"2022-07-20T11:16:43.79417Z","shell.execute_reply.started":"2022-07-20T11:16:43.787268Z","shell.execute_reply":"2022-07-20T11:16:43.793191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_lens = [len(t.split()) for t in texts.values]\nlen_mean = np.mean(text_lens)","metadata":{"execution":{"iopub.status.busy":"2022-07-20T11:16:46.567347Z","iopub.execute_input":"2022-07-20T11:16:46.568432Z","iopub.status.idle":"2022-07-20T11:16:46.67036Z","shell.execute_reply.started":"2022-07-20T11:16:46.56832Z","shell.execute_reply":"2022-07-20T11:16:46.669491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 1, figsize=(15, 8))\naxes[0].set_title('Distribution of number of tokens in tweets')\nsns.boxplot(text_lens, ax=axes[0])\nsns.histplot(text_lens, bins=100, kde=True, ax=axes[1])\naxes[1].vlines(len_mean, 0, 5000, color = 'r')\nplt.annotate(\"mean\", xy=(len_mean, 5000), xytext=(len_mean-2, 5050),\n            color='r')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T11:16:48.997432Z","iopub.execute_input":"2022-07-20T11:16:48.998396Z","iopub.status.idle":"2022-07-20T11:16:50.085825Z","shell.execute_reply.started":"2022-07-20T11:16:48.998359Z","shell.execute_reply":"2022-07-20T11:16:50.084835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Mean length of tweets is nearly 23 tokens. The distribution is scewed positively and a few outliers are observed on the right tail. Some of them are clode to the right whisker but some are located far from the majority of points. Let investigate them!","metadata":{}},{"cell_type":"code","source":"extreme_outliers = data['text'][np.array(text_lens) > 125]","metadata":{"execution":{"iopub.status.busy":"2022-07-20T11:16:54.023731Z","iopub.execute_input":"2022-07-20T11:16:54.024531Z","iopub.status.idle":"2022-07-20T11:16:54.038622Z","shell.execute_reply.started":"2022-07-20T11:16:54.024481Z","shell.execute_reply":"2022-07-20T11:16:54.037668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for idx in extreme_outliers.index:\n    print(idx, 'Target', data[TARGET][idx])\n    print(extreme_outliers[idx])\n    print('=-=-=-=-=-=-=-=-'*4, '\\n')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-07-20T11:16:54.75919Z","iopub.execute_input":"2022-07-20T11:16:54.759993Z","iopub.status.idle":"2022-07-20T11:16:54.768727Z","shell.execute_reply.started":"2022-07-20T11:16:54.759951Z","shell.execute_reply":"2022-07-20T11:16:54.767622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It interesting how tweets like \"######...\" become Nagative labeled. Some of these tweets do not contain any textual information. Such precendents could be removed. Some have both meaningful part and plenty of punctuation characters. They could be removed on the preprocessing stage.","metadata":{}},{"cell_type":"markdown","source":"Lets investigate outliers points which are closer to the majotiry.","metadata":{}},{"cell_type":"code","source":"outliers = data['text'][np.array(text_lens) > 60]","metadata":{"execution":{"iopub.status.busy":"2022-07-20T11:17:00.114808Z","iopub.execute_input":"2022-07-20T11:17:00.116217Z","iopub.status.idle":"2022-07-20T11:17:00.131109Z","shell.execute_reply.started":"2022-07-20T11:17:00.116155Z","shell.execute_reply":"2022-07-20T11:17:00.13011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for idx in outliers.index:\n    print(idx, 'Target', data[TARGET][idx])\n    print(outliers[idx])\n    print('=-=-=-=-=-=-=-=-'*4, '\\n')","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some quick notes:\n1. Most of the tweets seem to be meaningful\n2. I am concerned about the tweets with repeat words. For instance, I've met a few tweets which containe repeated \"Italy\"","metadata":{}},{"cell_type":"markdown","source":"### Target Analysis","metadata":{}},{"cell_type":"code","source":"# balance\ntarget_balance = data[TARGET].value_counts()\n\nplt.figure(figsize=(5, 5))\nplt.pie(target_balance, labels=[f'{idx}\\n{round(target_balance[idx]/len(data), 2)}' for idx in target_balance.index], \n        colors=['r', '#00FF00', '#FFFF00', 'gray'])\nplt.title('Proportions of target classes')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T11:17:06.382057Z","iopub.execute_input":"2022-07-20T11:17:06.382885Z","iopub.status.idle":"2022-07-20T11:17:06.496463Z","shell.execute_reply.started":"2022-07-20T11:17:06.382841Z","shell.execute_reply":"2022-07-20T11:17:06.495462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Target categories are almost of equal size, so resampling is not required.","metadata":{}},{"cell_type":"code","source":"# top words\nimport re\nfrom nltk.corpus import stopwords\nstopwords_list = stopwords.words('english')\n\nword_counts = {'Positive': [],\n                'Neutral': [],\n                'Irrelevant': [],\n                'Negative': []}\n\npattern = re.compile('[^\\w ]')\nfor text, t in zip(data['text'], data[TARGET]):\n    text = re.sub(pattern, '', text).lower().split()\n    text = [word for word in text if word not in stopwords_list]\n    word_counts[t].extend(text)\n               ","metadata":{"execution":{"iopub.status.busy":"2022-07-20T11:17:11.101105Z","iopub.execute_input":"2022-07-20T11:17:11.101749Z","iopub.status.idle":"2022-07-20T11:17:14.207902Z","shell.execute_reply.started":"2022-07-20T11:17:11.101709Z","shell.execute_reply":"2022-07-20T11:17:14.206713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 2, figsize=(20,10.5))\nfor axis, (target, words) in zip(axes.flatten(), word_counts.items()):\n    bar_info = pd.Series(words).value_counts()[:25]\n    sns.barplot(x=bar_info.values, y=bar_info.index, ax=axis)\n    axis.set_title(f'Top words for {target}')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T11:17:14.209919Z","iopub.execute_input":"2022-07-20T11:17:14.210359Z","iopub.status.idle":"2022-07-20T11:17:15.720935Z","shell.execute_reply.started":"2022-07-20T11:17:14.210318Z","shell.execute_reply":"2022-07-20T11:17:15.71976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see a clear difference between emotinal and neutral topics. In \"Positive\" top we can see some positive words, like *love, good, best, great*. In \"Negative\", instead, we observe lots of swear words. In neutral categories words with positive connotation are also observed, but not so frequent. To sum up, distribution of top-frequent words is different across target categories.","metadata":{}},{"cell_type":"markdown","source":"Now, lets see most correlated words for each topic using chi2.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf = TfidfVectorizer(sublinear_tf=True, min_df=5,\n                        ngram_range=(1, 2), \n                        stop_words='english',\n                        max_features=10000)\n\n\nfeatures = tfidf.fit_transform(data['text']).toarray()\n\nlabels = data[TARGET]\n\nprint(\"Each of the %d Text is represented by %d features (TF-IDF score of unigrams and bigrams)\" %(features.shape))","metadata":{"execution":{"iopub.status.busy":"2022-07-20T11:17:21.228957Z","iopub.execute_input":"2022-07-20T11:17:21.229434Z","iopub.status.idle":"2022-07-20T11:17:26.594942Z","shell.execute_reply.started":"2022-07-20T11:17:21.229394Z","shell.execute_reply":"2022-07-20T11:17:26.593834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import chi2\nN = 10\nfor label in set(labels):\n    features_chi2 = chi2(features, labels == label)\n    indices = np.argsort(features_chi2[0])\n    feature_names = np.array(tfidf.get_feature_names())[indices]\n    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n    print(\"\\n==> %s:\" %(label))\n    print(\"  * Most Correlated Unigrams are: %s\" %(', '.join(unigrams[-N:])))\n    print(\"  * Most Correlated Bigrams are: %s\" %(', '.join(bigrams[-N:])))","metadata":{"execution":{"iopub.status.busy":"2022-07-20T11:17:26.596934Z","iopub.execute_input":"2022-07-20T11:17:26.597368Z","iopub.status.idle":"2022-07-20T11:17:35.557572Z","shell.execute_reply.started":"2022-07-20T11:17:26.597325Z","shell.execute_reply":"2022-07-20T11:17:35.556608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# len for classes without outliers\ntweets_len = {'Positive': [],\n                'Neutral': [],\n                'Irrelevant': [],\n                'Negative': []}\npattern = re.compile('[^\\w ]')\ntweets_len = pd.DataFrame([len(re.sub(pattern, '', text).lower().split()) for text in data['text'] if len(text)< 125],\n                         columns=['len'])","metadata":{"execution":{"iopub.status.busy":"2022-07-20T11:17:35.559728Z","iopub.execute_input":"2022-07-20T11:17:35.560441Z","iopub.status.idle":"2022-07-20T11:17:35.85965Z","shell.execute_reply.started":"2022-07-20T11:17:35.560393Z","shell.execute_reply":"2022-07-20T11:17:35.858793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets_len['target'] = data[TARGET]","metadata":{"execution":{"iopub.status.busy":"2022-07-20T11:17:35.861208Z","iopub.execute_input":"2022-07-20T11:17:35.861652Z","iopub.status.idle":"2022-07-20T11:17:35.871358Z","shell.execute_reply.started":"2022-07-20T11:17:35.861611Z","shell.execute_reply":"2022-07-20T11:17:35.86966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20, 7))\nsns.kdeplot(data=tweets_len, x='len', hue='target')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T11:17:35.873937Z","iopub.execute_input":"2022-07-20T11:17:35.874492Z","iopub.status.idle":"2022-07-20T11:17:36.575208Z","shell.execute_reply.started":"2022-07-20T11:17:35.874454Z","shell.execute_reply":"2022-07-20T11:17:36.574133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distributions of tweets length do not differ from each other. We can ensure using ANOVA test as we have equal dispesion and normal-like distribution.","metadata":{}},{"cell_type":"code","source":"import statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\n#perform two-way ANOVA\nmodel = ols('len ~ target', data=tweets_len).fit()\nsm.stats.anova_lm(model, typ=2)","metadata":{"execution":{"iopub.status.busy":"2022-07-20T11:17:40.969766Z","iopub.execute_input":"2022-07-20T11:17:40.970171Z","iopub.status.idle":"2022-07-20T11:17:41.842262Z","shell.execute_reply.started":"2022-07-20T11:17:40.970138Z","shell.execute_reply":"2022-07-20T11:17:41.841242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"p-value > 0.05, fail to reject H0.","metadata":{}},{"cell_type":"code","source":"# emoticons\nimport emoji\nimport regex as re\n\ndef split_count(text):\n    emoji_list = []\n    data = re.findall(r'\\X', text)\n    for word in data:\n        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n            emoji_list.append(word)\n    \n    return emoji_list","metadata":{"execution":{"iopub.status.busy":"2022-07-20T11:17:46.442055Z","iopub.execute_input":"2022-07-20T11:17:46.442533Z","iopub.status.idle":"2022-07-20T11:17:46.479547Z","shell.execute_reply.started":"2022-07-20T11:17:46.442497Z","shell.execute_reply":"2022-07-20T11:17:46.478718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_emojis = {'Positive': [],\n                'Neutral': [],\n                'Irrelevant': [],\n                'Negative': []}\n\npattern = re.compile('\\u200d')\nfor i, text in enumerate(texts):\n    emoji_count = split_count(text)\n    if emoji_count:\n        emoji_count = [re.sub(pattern, '', e) for e in emoji_count]\n        target_emojis[data[TARGET].iloc[i]].extend(emoji_count)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-07-20T11:17:48.814416Z","iopub.execute_input":"2022-07-20T11:17:48.815002Z","iopub.status.idle":"2022-07-20T11:18:06.131498Z","shell.execute_reply.started":"2022-07-20T11:17:48.814951Z","shell.execute_reply":"2022-07-20T11:18:06.130567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for t, emojis in target_emojis.items():\n    plt.figure(figsize=(10, 5))\n    bar_info = pd.Series(emojis).value_counts()[:20]\n    print('=========='*10,  f'\\nTop emojis for {t} \\n', list(bar_info.index))\n    bar_info.index = [emoji.demojize(i, delimiters=(\"\", \"\")) for i in bar_info.index]\n    sns.barplot(x=bar_info.values, y=bar_info.index)\n        \n    plt.title(f'{t}')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T11:18:06.133036Z","iopub.execute_input":"2022-07-20T11:18:06.13352Z","iopub.status.idle":"2022-07-20T11:18:07.680974Z","shell.execute_reply.started":"2022-07-20T11:18:06.133484Z","shell.execute_reply":"2022-07-20T11:18:07.680085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, the most popular emoji across al the classes is rolling_on_the_floor_laughing except Negative. Here, person_facepalming is on the 1st place. For Positive class, for instance, it is on the 4th. Also, all other emoticons have different distribution across the taget cetegories. Also, It is noticeable that the gap between the most popular (rolling_on_the_floor_laughing) and the second popular emoticons is different and is the smallest for Negative class. \n\nNegative class also contains some negative emojis like 🤬, 🥴, 🤷 which do not occur in other categories.","metadata":{}},{"cell_type":"code","source":"# capitalization distribution\ncapitalized = [np.sum([t.isupper() for t in text.split()]) for text in np.array(data['text'])]","metadata":{"execution":{"iopub.status.busy":"2022-07-20T11:18:15.052795Z","iopub.execute_input":"2022-07-20T11:18:15.053745Z","iopub.status.idle":"2022-07-20T11:18:15.97109Z","shell.execute_reply.started":"2022-07-20T11:18:15.053699Z","shell.execute_reply":"2022-07-20T11:18:15.970173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"capitalized_target = pd.DataFrame([(c, t) for c, t in zip(capitalized, data[TARGET])], columns=['cap', 'target'])","metadata":{"execution":{"iopub.status.busy":"2022-07-20T11:18:15.972866Z","iopub.execute_input":"2022-07-20T11:18:15.97327Z","iopub.status.idle":"2022-07-20T11:18:16.050497Z","shell.execute_reply.started":"2022-07-20T11:18:15.973217Z","shell.execute_reply":"2022-07-20T11:18:16.049544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 1, figsize=(15, 8))\naxes[0].set_title('Distribution of capitalized in tweets')\nsns.boxplot(x=capitalized_target['cap'], y=capitalized_target['target'], ax=axes[0])\n\nsns.kdeplot(data=capitalized_target, x='cap', hue='target', ax=axes[1])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T11:18:18.028295Z","iopub.execute_input":"2022-07-20T11:18:18.028993Z","iopub.status.idle":"2022-07-20T11:18:19.537806Z","shell.execute_reply.started":"2022-07-20T11:18:18.028945Z","shell.execute_reply":"2022-07-20T11:18:19.536944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# closer look\ncapitalized_target_no_outliers = capitalized_target[capitalized_target['cap'] < 75]\nfig, axes = plt.subplots(2, 1, figsize=(15, 8))\naxes[0].set_title('Distribution of capitalized in tweets')\nsns.boxplot(x=capitalized_target_no_outliers['cap'], y=capitalized_target['target'], ax=axes[0])\n\nsns.kdeplot(data=capitalized_target_no_outliers, x='cap', hue='target', ax=axes[1])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T11:18:26.120022Z","iopub.execute_input":"2022-07-20T11:18:26.120923Z","iopub.status.idle":"2022-07-20T11:18:27.309906Z","shell.execute_reply.started":"2022-07-20T11:18:26.12088Z","shell.execute_reply":"2022-07-20T11:18:27.309092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5, 8))\nsns.heatmap(pd.crosstab(data['refers to'], data[TARGET], normalize='index'), annot=True)\nplt.title('Frequencies of meeting referred objects in each category')","metadata":{"execution":{"iopub.status.busy":"2022-07-20T11:18:39.83699Z","iopub.execute_input":"2022-07-20T11:18:39.837416Z","iopub.status.idle":"2022-07-20T11:18:40.753705Z","shell.execute_reply.started":"2022-07-20T11:18:39.837379Z","shell.execute_reply":"2022-07-20T11:18:40.752882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"markdown","source":"Sentiment information is mostly expressed on the lexical and punctuational levels. For internet discourse we can also use emoticons as their main purpose is to represent emotions which are our target.\n\nThus, syntactic relations are not really important to us. As well as semantic features of words. So, we can use simple tf-idf encoders in which we also enclude punctuation (mostly exclamation and question marks), case (CapsLoc as a signal of strong emotions) and emojis.\n\nRequired preparation steps:\n1. lowercase words, but count for each text how many characters or words we capitalized.\n1. remove stopwords and numbers as sentiment neutral\n1. decode emojis\n1. lemmatize\n1. vectorize with tf-idf.\n1. Add refers to feature ohe\n1. delete nans\n\nAdditionally perform feature selection with chi2","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom spacy.lang.en import English\nimport emoji\nimport spacy\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.utils.validation import check_is_fitted\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.exceptions import NotFittedError\n\nimport re\n\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nstopwords = set(stopwords.words('english'))\nnlp = spacy.load(\"en_core_web_sm\")","metadata":{"execution":{"iopub.status.busy":"2022-07-20T11:18:47.087905Z","iopub.execute_input":"2022-07-20T11:18:47.088691Z","iopub.status.idle":"2022-07-20T11:18:52.986899Z","shell.execute_reply.started":"2022-07-20T11:18:47.088647Z","shell.execute_reply":"2022-07-20T11:18:52.98598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ids_to_remove = [1826, 10454, 32186, 68078]\ndata = data[~data.index.isin(ids_to_remove)]\ndata.index = range(len(data))","metadata":{"execution":{"iopub.status.busy":"2022-07-20T11:18:53.996527Z","iopub.execute_input":"2022-07-20T11:18:53.997429Z","iopub.status.idle":"2022-07-20T11:18:54.009746Z","shell.execute_reply.started":"2022-07-20T11:18:53.997384Z","shell.execute_reply":"2022-07-20T11:18:54.008648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Preprocessor:\n    def __init__(self, stopwords=stopwords):\n        self.vectorizer = TfidfVectorizer(lowercase=False, max_features=8000,\n                                         min_df=10, ngram_range=(1, 3),\n                                         tokenizer=None)\n        self.stopwords = stopwords\n        self.vectorizer_fitted = False\n        \n    def remove_urls(self, texts):\n        print('Removing URLs...')\n        pattern = re.compile('(\\w+\\.com ?/ ?.+)|(http\\S+)')\n        return [re.sub(pattern, '', text) for text in texts]\n    \n    def remove_double_space(self, texts):\n        print('Removing double space...')\n        pattern = re.compile(' +')\n        return [re.sub(pattern, ' ', text) for text in texts]\n        \n    def remove_punctuation(self, texts):\n        print('Removing Punctuation...')\n        pattern = re.compile('[^a-z ]')\n        return [re.sub(pattern, ' ', text) for text in texts]\n    \n    def remove_stopwords(self, texts):\n        print('Removing stopwords...')\n        return [[w for w in text.split(' ') if w not in self.stopwords] for text in tqdm(texts)]\n    \n    def remove_numbers(self, texts):\n        print('Removing numbers...')\n        return [' '.join([w for w in text if not w.isdigit()]) for text in tqdm(texts)]\n    \n    def decode_emojis(self, texts):\n        print('Decoding emojis...')\n        return [emoji.demojize(text, language='en') for text in texts] \n    \n    def lemmatize(self, texts):\n        print('Lemmatizing...')\n        lemmatized_texts = []\n        for text in tqdm(texts):\n            doc = nlp(text)\n            lemmatized_texts.append(' '.join([token.lemma_ for token in doc]))\n                                    \n        return lemmatized_texts\n        \n    def transform(self, X, y=None, mode='train'):\n        X = X.copy()\n        print('Removing Nans...')\n        X = X[~X.isnull()]                          # delete nans\n        X = X[~X.duplicated()]                      # delete duplicates\n        \n        if mode == 'train':\n            self.train_idx = X.index\n        else:\n            self.test_idx = X.index\n        print('Counting capitalized...')\n        capitalized = [np.sum([t.isupper() for t in text.split()]) \n                           for text in np.array(X.values)]  # count capitalized\n        # X['cap'] = capitalized\n        print('Lowering...')\n        X = [text.lower() for text in X]             # lower\n        X = self.remove_urls(X)                      # remove urls\n        X = self.remove_punctuation(X)               # remove punctuation\n        X = self.remove_double_space(X)              # remove double space\n        X = self.decode_emojis(X)                    # decode emojis\n        X = self.remove_stopwords(X)                 # remove stopwords\n        X = self.remove_numbers(X)                   # remove numbers                      \n        X = self.lemmatize(X)                        # lemmatize\n        \n        if not self.vectorizer_fitted:\n            self.vectorizer_fitted = True\n            print('Fitting vectorizer...')\n            self.vectorizer.fit(X)\n\n        print('Vectorizing...')\n        X = self.vectorizer.transform(X)             # vectorize\n        \n        return X","metadata":{"execution":{"iopub.status.busy":"2022-07-20T10:42:17.081688Z","iopub.execute_input":"2022-07-20T10:42:17.082061Z","iopub.status.idle":"2022-07-20T10:42:17.10412Z","shell.execute_reply.started":"2022-07-20T10:42:17.082027Z","shell.execute_reply":"2022-07-20T10:42:17.102922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pr = Preprocessor()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T10:42:19.430072Z","iopub.execute_input":"2022-07-20T10:42:19.430798Z","iopub.status.idle":"2022-07-20T10:42:19.435927Z","shell.execute_reply.started":"2022-07-20T10:42:19.430758Z","shell.execute_reply":"2022-07-20T10:42:19.434284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train = data\ndata_test = val_data\n\ny_train = data['sentiment']\ny_test = val_data['sentiment']","metadata":{"execution":{"iopub.status.busy":"2022-07-20T10:42:22.11078Z","iopub.execute_input":"2022-07-20T10:42:22.111582Z","iopub.status.idle":"2022-07-20T10:42:22.118015Z","shell.execute_reply.started":"2022-07-20T10:42:22.111533Z","shell.execute_reply":"2022-07-20T10:42:22.117024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train_pr = pr.transform(data_train['text'])\ndata_train_pr = pd.DataFrame.sparse.from_spmatrix(data_train_pr, columns=pr.vectorizer.get_feature_names_out())\n\nohe = OneHotEncoder()\nreferring_ohe = ohe.fit_transform(data_train['refers to'][data_train.index.isin(pr.train_idx)].to_numpy().reshape(-1, 1))\nreferring_ohe = pd.DataFrame.sparse.from_spmatrix(referring_ohe, columns=ohe.get_feature_names_out())\n\nX_train = pd.concat([data_train_pr, referring_ohe], axis=1)\ny_train = y_train[y_train.index.isin(pr.train_idx)]\ny_train.index = X_train.index","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_test_pr = pr.transform(data_test['text'], mode='test')\ndata_test_pr = pd.DataFrame.sparse.from_spmatrix(data_test_pr, columns=pr.vectorizer.get_feature_names_out())\n\nohe = OneHotEncoder()\nreferring_ohe = ohe.fit_transform(data_train['refers to'][data_train.index.isin(pr.train_idx)].to_numpy().reshape(-1, 1))\nreferring_ohe = ohe.transform(data_test['refers to'][data_test.index.isin(pr.test_idx)].to_numpy().reshape(-1, 1))\nreferring_ohe = pd.DataFrame.sparse.from_spmatrix(referring_ohe, columns=ohe.get_feature_names_out())\n\nX_test = pd.concat([data_test_pr, referring_ohe], axis=1)\ny_test = y_test[y_test.index.isin(pr.test_idx)]\ny_test.index = X_test.index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-07-20T11:19:16.325179Z","iopub.execute_input":"2022-07-20T11:19:16.325883Z","iopub.status.idle":"2022-07-20T11:19:16.331986Z","shell.execute_reply.started":"2022-07-20T11:19:16.32584Z","shell.execute_reply":"2022-07-20T11:19:16.331133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save prepared data for the future\n\nwith open('../X_train.pkl', 'wb') as f:\n    pickle.dump(X_train, f)\nwith open('../X_test.pkl', 'wb') as f:\n    pickle.dump(X_test, f)\n\nwith open('../y_train.pkl', 'wb') as f:\n    pickle.dump(y_train, f)\nwith open('../y_test.pkl', 'wb') as f:\n    pickle.dump(y_test, f)\n    \n# with open('../X_train.pkl', 'rb') as f:\n#     X_train = pickle.load(f)\n# with open('../X_test.pkl', 'rb') as f:\n#     X_test = pickle.load(f)\n# with open('../y_train.pkl', 'rb') as f:\n#     y_train = pickle.load(f)\n# with open('../y_test.pkl', 'rb') as f:\n#     y_test = pickle.load(f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom scipy.stats import uniform, randint\nfrom sklearn.model_selection import train_test_split, GridSearchCV, KFold\nfrom sklearn.metrics import confusion_matrix","metadata":{"execution":{"iopub.status.busy":"2022-07-20T11:10:57.162528Z","iopub.execute_input":"2022-07-20T11:10:57.163542Z","iopub.status.idle":"2022-07-20T11:10:57.838038Z","shell.execute_reply.started":"2022-07-20T11:10:57.163501Z","shell.execute_reply":"2022-07-20T11:10:57.837128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_cv(model, X_train, y_train, params, n_splits=5, scoring='f1_weighted'):\n    kf = KFold(n_splits=n_splits, random_state=0, shuffle=True)\n\n    cv = RandomizedSearchCV(model,\n                        params,\n                        cv=kf,\n                        scoring=scoring,\n                        return_train_score=True,\n                        n_jobs=-1,\n                        verbose=2,\n                        random_state=1\n                        )\n    cv.fit(X_train, y_train)\n\n    print('Best params', cv.best_params_)\n    return cv","metadata":{"execution":{"iopub.status.busy":"2022-07-20T08:14:03.877186Z","iopub.execute_input":"2022-07-20T08:14:03.877676Z","iopub.status.idle":"2022-07-20T08:14:03.892326Z","shell.execute_reply.started":"2022-07-20T08:14:03.877638Z","shell.execute_reply":"2022-07-20T08:14:03.891513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rs_parameters = {\n    'penalty': ['l2', 'l1', 'elasticnet'],\n    'C': uniform(scale=10),\n    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'saga'],\n    'l1_ratio': uniform(scale=10)\n    }","metadata":{"execution":{"iopub.status.busy":"2022-07-20T08:14:07.267496Z","iopub.execute_input":"2022-07-20T08:14:07.267941Z","iopub.status.idle":"2022-07-20T08:14:07.277668Z","shell.execute_reply.started":"2022-07-20T08:14:07.267902Z","shell.execute_reply":"2022-07-20T08:14:07.275085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. Without feature selection\n2. MI feature selection","metadata":{}},{"cell_type":"markdown","source":"## Training without feature selection","metadata":{}},{"cell_type":"code","source":"lr = LogisticRegression()\nmodel_cv_lr = train_cv(lr, X_train, y_train, rs_parameters)\n\nbestimator_lr = model_cv_lr.best_estimator_","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Best params {'C': 4.17022004702574, 'l1_ratio': 7.203244934421581, 'penalty': 'l1', 'solver': 'saga'}","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_test, bestimator_lr.predict(X_test)))","metadata":{"execution":{"iopub.status.busy":"2022-07-20T09:00:48.224349Z","iopub.execute_input":"2022-07-20T09:00:48.224705Z","iopub.status.idle":"2022-07-20T09:00:48.528995Z","shell.execute_reply.started":"2022-07-20T09:00:48.224676Z","shell.execute_reply":"2022-07-20T09:00:48.528052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nsns.heatmap(confusion_matrix(y_test, bestimator_lr.predict(X_test)), annot=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T09:00:36.189787Z","iopub.execute_input":"2022-07-20T09:00:36.190275Z","iopub.status.idle":"2022-07-20T09:00:36.900308Z","shell.execute_reply.started":"2022-07-20T09:00:36.190193Z","shell.execute_reply":"2022-07-20T09:00:36.899514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pretty descend score for a baseline. All classes are predicted almost equally good, confusion matrix look diagonal.\n\nLet's look if we could get the same score after droping some the least important features according to Mutual Information score.","metadata":{}},{"cell_type":"markdown","source":"## With MI feature selection","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import mutual_info_classif as MIC\nmi_score = MIC(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2022-07-20T11:23:44.744867Z","iopub.execute_input":"2022-07-20T11:23:44.745892Z","iopub.status.idle":"2022-07-20T11:40:43.15864Z","shell.execute_reply.started":"2022-07-20T11:23:44.745849Z","shell.execute_reply":"2022-07-20T11:40:43.156607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols_importance = sorted(list(zip(X_train.columns, mi_score)), key=lambda x: x[1], reverse=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-20T11:40:43.161729Z","iopub.execute_input":"2022-07-20T11:40:43.162642Z","iopub.status.idle":"2022-07-20T11:40:43.182709Z","shell.execute_reply.started":"2022-07-20T11:40:43.162598Z","shell.execute_reply":"2022-07-20T11:40:43.181667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 8))\nmi_imp = [pair[1] for pair in cols_importance[:30]]\ncols = [pair[0] for pair in cols_importance[:30]]\nsns.barplot(x=mi_imp, y=cols)\nplt.title('The most important features')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T11:40:43.184294Z","iopub.execute_input":"2022-07-20T11:40:43.184755Z","iopub.status.idle":"2022-07-20T11:40:43.561052Z","shell.execute_reply.started":"2022-07-20T11:40:43.184712Z","shell.execute_reply":"2022-07-20T11:40:43.560054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 8))\nmi_imp = [pair[1] for pair in cols_importance[-30:]]\ncols = [pair[0] for pair in cols_importance[-30:]]\nsns.barplot(x=mi_imp, y=cols)\nplt.title('The least important features')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T11:43:22.589856Z","iopub.execute_input":"2022-07-20T11:43:22.590796Z","iopub.status.idle":"2022-07-20T11:43:22.968799Z","shell.execute_reply.started":"2022-07-20T11:43:22.590738Z","shell.execute_reply":"2022-07-20T11:43:22.967943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_6k = X_train[[pair[0] for pair in cols_importance[:6000]]]\nX_test_6k = X_test[[pair[0] for pair in cols_importance[:6000]]]","metadata":{"execution":{"iopub.status.busy":"2022-07-20T11:40:44.123785Z","iopub.execute_input":"2022-07-20T11:40:44.126941Z","iopub.status.idle":"2022-07-20T11:40:44.697118Z","shell.execute_reply.started":"2022-07-20T11:40:44.126893Z","shell.execute_reply":"2022-07-20T11:40:44.696241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save for the future\n\n# with open('../X_train_6k.pkl', 'wb') as f:\n#     pickle.dump(X_train_6k, f)\n# with open('../X_test_6k.pkl', 'wb') as f:\n#     pickle.dump(X_test_6k, f)\n    \n# with open('../X_train_6k.pkl', 'rb') as f:\n#     X_train_6k = pickle.load(f)\n# with open('../X_test_6k.pkl', 'rb') as f:\n#     X_test_6k = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2022-07-19T14:20:18.527007Z","iopub.execute_input":"2022-07-19T14:20:18.527409Z","iopub.status.idle":"2022-07-19T14:20:19.902703Z","shell.execute_reply.started":"2022-07-19T14:20:18.527375Z","shell.execute_reply":"2022-07-19T14:20:19.901809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# leave 6k features\nlr = LogisticRegression()\nmodel_cv_lr_6k = train_cv(lr, X_train_6k, y_train, rs_parameters)\n\nbestimator_lr_6k = model_cv_lr_6k.best_estimator_","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Best params {'C': 9.13962024579233, 'l1_ratio': 4.572048079869883, 'penalty': 'l1', 'solver': 'saga'}","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_test, bestimator_lr_6k.predict(X_test_6k)))","metadata":{"execution":{"iopub.status.busy":"2022-07-20T10:14:20.05879Z","iopub.execute_input":"2022-07-20T10:14:20.06136Z","iopub.status.idle":"2022-07-20T10:14:20.313033Z","shell.execute_reply.started":"2022-07-20T10:14:20.061314Z","shell.execute_reply":"2022-07-20T10:14:20.312096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(confusion_matrix(y_test, bestimator_lr_6k.predict(X_test_6k)), annot=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T10:14:20.314671Z","iopub.execute_input":"2022-07-20T10:14:20.315465Z","iopub.status.idle":"2022-07-20T10:14:20.909809Z","shell.execute_reply.started":"2022-07-20T10:14:20.315414Z","shell.execute_reply":"2022-07-20T10:14:20.909055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The scores dropped, but not dramatically. The confusion matrix is still diagonal.\n\nTo sum up, with 75% of features we can obtain 97% of baseline accuracy with every feature.","metadata":{}},{"cell_type":"markdown","source":"## Binary Classification + Textblob sentiment analysis","metadata":{}},{"cell_type":"markdown","source":"Textblob.sentiment is a pretrained instrument which return the polarity and subjectivity of a text.\n\nIn the task, we need not only predict sentiment, but also detect irrelevant messages.\n\nIn this section, I created a SentimntClassifier which contains a Logistic regression for binary classification: relevant and irrelevant messages. Then relevant texts are passed to textblob.sentiment for polarity estimating.","metadata":{}},{"cell_type":"code","source":"from textblob import TextBlob","metadata":{"execution":{"iopub.status.busy":"2022-07-20T09:13:46.731049Z","iopub.execute_input":"2022-07-20T09:13:46.731403Z","iopub.status.idle":"2022-07-20T09:13:46.773445Z","shell.execute_reply.started":"2022-07-20T09:13:46.731374Z","shell.execute_reply":"2022-07-20T09:13:46.772604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SentimentClassifier:\n    def __init__(self):\n        self.classifier = LogisticRegression()\n    \n    def fit_classifier(self, X_vector, y, params):\n        model_cv = self.classifier.fit(X_vector, y)\n        self.classifier = model_cv#.best_estimator_\n        \n    def __return_label(self, polarity):\n        if -1. <= polarity < 0:\n            return 'Negative'\n        if polarity == 0:\n            return 'Neutral'\n        return 'Positive'\n        \n    def predict_sentiment(self, X_vector, X_texts):\n        irrelevance = self.classifier.predict(X_vector)\n        X_sentimental = X_texts[irrelevance == 0]\n        X_sentimental_idx = X_sentimental.index\n        X_irrelevant_index = X_vector[irrelevance == 1].index\n        \n        pred_sent = [TextBlob(text).sentiment.polarity for text in X_sentimental]\n        pred_sent = pd.Series([*map(lambda x: self.__return_label(x), pred_sent )],\n                              index=X_sentimental_idx)\n        \n        pred_irrelevance = irrelevance[irrelevance == 1]\n        pred_irrelevance = pd.Series(pred_irrelevance, index=X_irrelevant_index)\n        pred_irrelevance = pred_irrelevance.apply(lambda x: 'Irrelevant')\n        \n        pred = pd.concat([pred_irrelevance, pred_sent], axis=0).sort_index(inplace=False)\n        \n        return pred","metadata":{"execution":{"iopub.status.busy":"2022-07-20T09:13:47.128068Z","iopub.execute_input":"2022-07-20T09:13:47.128401Z","iopub.status.idle":"2022-07-20T09:13:47.13841Z","shell.execute_reply.started":"2022-07-20T09:13:47.128374Z","shell.execute_reply":"2022-07-20T09:13:47.137388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sent_classifier = SentimentClassifier()\n\nsent_classifier.fit_classifier(X_train_5k, \n                               [1 if target == 'Irrelevant' else 0 for target in y_train], \n                               rs_parameters)\n\nX_texts_test = data['text'][data.index.isin(X_test.index)]","metadata":{"execution":{"iopub.status.busy":"2022-07-20T09:13:49.186079Z","iopub.execute_input":"2022-07-20T09:13:49.186919Z","iopub.status.idle":"2022-07-20T09:13:50.148954Z","shell.execute_reply.started":"2022-07-20T09:13:49.186882Z","shell.execute_reply":"2022-07-20T09:13:50.148193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_test = sent_classifier.predict_sentiment(X_test_5k, X_texts_test)\npred_test.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T09:13:50.277299Z","iopub.execute_input":"2022-07-20T09:13:50.27783Z","iopub.status.idle":"2022-07-20T09:13:51.427933Z","shell.execute_reply.started":"2022-07-20T09:13:50.277796Z","shell.execute_reply":"2022-07-20T09:13:51.427057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, pred_test))","metadata":{"execution":{"iopub.status.busy":"2022-07-20T09:13:52.033742Z","iopub.execute_input":"2022-07-20T09:13:52.034393Z","iopub.status.idle":"2022-07-20T09:13:52.077845Z","shell.execute_reply.started":"2022-07-20T09:13:52.034358Z","shell.execute_reply":"2022-07-20T09:13:52.076933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(confusion_matrix(y_test, pred_test), annot=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T09:14:22.355226Z","iopub.execute_input":"2022-07-20T09:14:22.355845Z","iopub.status.idle":"2022-07-20T09:14:22.727743Z","shell.execute_reply.started":"2022-07-20T09:14:22.355799Z","shell.execute_reply":"2022-07-20T09:14:22.727028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"TextBlob seems to cope poorly with the given task. Meanwhile, not optimized logistic regression managed to classify massages much better than textblob classified later relevant ones.\n\nThe confusion matrix is a mess. All classes are mostly detected as the Positive one.\n\nI consider this experiment as a fail and I am not intended to optimize it later.","metadata":{}},{"cell_type":"markdown","source":"## Pytorch Lightning classifier","metadata":{}},{"cell_type":"markdown","source":"In this section I am bilding a neural network using pytorch lightning.","metadata":{}},{"cell_type":"code","source":"import pickle\nwith open('../X_train.pkl', 'rb') as f:\n    X_train = pickle.load(f)\nwith open('../X_test.pkl', 'rb') as f:\n    X_test = pickle.load(f)\nwith open('../y_train.pkl', 'rb') as f:\n    y_train = pickle.load(f)\nwith open('../y_test.pkl', 'rb') as f:\n    y_test = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2022-07-20T10:59:24.207454Z","iopub.execute_input":"2022-07-20T10:59:24.208749Z","iopub.status.idle":"2022-07-20T10:59:25.280421Z","shell.execute_reply.started":"2022-07-20T10:59:24.208641Z","shell.execute_reply":"2022-07-20T10:59:25.279511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lightning Module\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import random_split\nimport pytorch_lightning as pl\nimport torch.utils.data as data_utils\nfrom torch.optim.lr_scheduler import StepLR\nfrom pytorch_lightning.callbacks import LearningRateMonitor\nfrom IPython.display import clear_output\nimport numpy as np\n\ntorch.manual_seed(42)\n\ndef plot_loss(losses, title=''):\n    \"\"\"\n    plots and saves loss progression while fitting\n    \"\"\"\n    if len(losses) < 3:\n        return\n\n    pos = np.vstack(losses)\n    x, y = pos.T\n    plt.clf()\n    plt.ion()\n    plt.figure(figsize=(9, 5))\n    plt.plot(x, y)\n    plt.title(title)\n    clear_output(wait=True)\n    plt.show()\n\nclass NNSentimentClassifier(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.softmax = nn.Softmax(dim=1)\n        self.dropout = nn.Dropout(0.2)\n        self.model = nn.Sequential(\n        nn.Linear(8032, 1000),\n        nn.ReLU(),\n        self.dropout,\n        nn.Linear(1000, 100),\n        nn.Tanh(),\n        self.dropout,\n        nn.Linear(100, 1000),\n        nn.ReLU(),\n        self.dropout,\n        nn.Linear(1000, 10),\n        nn.ReLU(),\n        self.dropout,\n        nn.Linear(10, 4)\n        )\n        self.acc_train_loss = []\n        self.acc_val_loss = []\n    \n    def forward (self, x):\n        prediction = self.model(x)\n        return prediction\n        \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=0.001, weight_decay=0.0001)\n        scheduler = StepLR(optimizer, step_size=3, gamma=0.1, verbose=True)\n        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n\n    def training_step(self, train_batch, batch_idx):\n        x, y = train_batch\n        y = y.long()\n        prediction = self.forward(x.float())\n        loss = F.cross_entropy(prediction, y)\n        self.log('train_loss', loss)\n        return loss\n    \n    def validation_step(self, val_batch, batch_idx):\n        x, y = val_batch\n        y = y.long()\n        prediction = self.forward(x.float())\n        loss = F.cross_entropy(prediction, y)\n        self.log('val_loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n    \n    def test_step(self, test_batch, batc_idx):\n        x, y = test_batch\n        y = y.long()\n        prediction = model.forward(x.float())\n        preds = torch.argmax(prediction, dim=1)\n\n        return preds","metadata":{"execution":{"iopub.status.busy":"2022-07-20T10:59:27.670365Z","iopub.execute_input":"2022-07-20T10:59:27.670936Z","iopub.status.idle":"2022-07-20T10:59:36.474979Z","shell.execute_reply.started":"2022-07-20T10:59:27.670898Z","shell.execute_reply":"2022-07-20T10:59:36.473882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data\ntarget_to_idx = {\n    'Irrelevant': 0,\n    'Negative': 1,\n    'Neutral': 2,\n    'Positive': 3\n    }\n\ny_train_idx = torch.from_numpy(y_train.map(target_to_idx).values.astype(np.float))\ntrain_data_tensor = data_utils.TensorDataset(torch.from_numpy(X_train.to_numpy().astype(np.float)), y_train_idx)\ntrain_data_tensor, val_data_tensor = random_split(train_data_tensor, [62540, 6948])\ntrain_loader = DataLoader(train_data_tensor, batch_size=32)\nval_loader = DataLoader(val_data_tensor, batch_size=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model\nmodel = NNSentimentClassifier()\n# training\n\ntrainer = pl.Trainer(gpus=1, precision=16, limit_train_batches=0.5, max_epochs=50)\ntrainer.fit(model, train_loader, val_loader)","metadata":{"execution":{"iopub.status.busy":"2022-07-20T10:59:43.124964Z","iopub.execute_input":"2022-07-20T10:59:43.125403Z","iopub.status.idle":"2022-07-20T11:09:41.332824Z","shell.execute_reply.started":"2022-07-20T10:59:43.125367Z","shell.execute_reply":"2022-07-20T11:09:41.331892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I've stopped training after noticing that validation score stopped dropping, but started increasing","metadata":{}},{"cell_type":"code","source":"y_test_idx = torch.from_numpy(y_test.map(target_to_idx).values.astype(np.float))\ntest_data_tensor = data_utils.TensorDataset(torch.from_numpy(X_test.to_numpy().astype(np.float)), y_test_idx)\ntest_loader = DataLoader(test_data_tensor, batch_size=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = []\nfor batch in test_loader:\n    x, y = batch\n    y = y.long()\n    prediction = model.forward(x.float().cuda())\n    preds.extend(torch.argmax(prediction, dim=1).cpu())","metadata":{"execution":{"iopub.status.busy":"2022-07-20T11:10:07.522531Z","iopub.execute_input":"2022-07-20T11:10:07.522972Z","iopub.status.idle":"2022-07-20T11:10:08.226104Z","shell.execute_reply.started":"2022-07-20T11:10:07.522925Z","shell.execute_reply":"2022-07-20T11:10:08.2252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(preds, y_test_idx))","metadata":{"execution":{"iopub.status.busy":"2022-07-20T11:11:33.593099Z","iopub.execute_input":"2022-07-20T11:11:33.594127Z","iopub.status.idle":"2022-07-20T11:11:33.621036Z","shell.execute_reply.started":"2022-07-20T11:11:33.593981Z","shell.execute_reply":"2022-07-20T11:11:33.619976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(confusion_matrix(preds, y_test_idx), annot=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T11:11:31.198947Z","iopub.execute_input":"2022-07-20T11:11:31.199459Z","iopub.status.idle":"2022-07-20T11:11:31.484081Z","shell.execute_reply.started":"2022-07-20T11:11:31.199419Z","shell.execute_reply":"2022-07-20T11:11:31.483157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The netwok allowed to increase test score not significantly: from 91% to 93% accuracy.\n\nThe class which is predicted less accurate than the others is the second (Neutral). And this could be understood from the nature of the classes where the \"middle position\" is not that destinctive as the polar positions.","metadata":{}},{"cell_type":"code","source":"# save the net \ntorch.save(model.state_dict(), 'net.pt')","metadata":{"execution":{"iopub.status.busy":"2022-07-20T11:15:15.503776Z","iopub.execute_input":"2022-07-20T11:15:15.50462Z","iopub.status.idle":"2022-07-20T11:15:15.599044Z","shell.execute_reply.started":"2022-07-20T11:15:15.504578Z","shell.execute_reply":"2022-07-20T11:15:15.598035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}